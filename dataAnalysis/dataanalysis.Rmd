---
title: "Data Analysis"
output: 
  html_document:
    toc: yes
    code_folding: show
    theme:
      bg: "#202123"
      fg: "#c8cbd0" # grey
      primary: "#00b36b"  # pink "#EA80FC" 
      base_font:
        google: Ubuntu
      heading_font:
        google: Ubuntu
      version: 3
---

The source of learning material is **Data Analysis in R** eBook.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (requireNamespace("thematic")) 
  thematic::thematic_rmd(font = "auto")
```



# Models & Estimation

Models are description of the system, process, or relationship you are trying to evaluate. If you have an accurate model for your system, then you will have a better idea of the quality of your data, the inference of your parameters, and generally get much farther in your analysis. 

response  <= (model or relationship being tested or confirmed) + stochastic


**stochastic** = randomly determined, random error or deviations 
from the relationship that are unexplained by the model


Example: data regarding rainfall and plant growth.

- response is $y$ plant growth
- data determines rainfall $x$ 
- hypothesis = increase in rainfall results in increase plant growth
- factors = sunlight, temperature etc.  
- factors and unknowns => random error (stochastic)

Simple Linear Regression: 
as x increase = change in y (constant across all values of x) 

findings:  increase in rainfall results in a small increase in plant growth
which estimates a parameter {coefficient}


## Model Complexity 

We want a model to be general enough that it contains truths and can be applied elsewhere, but not so general that it does not advance our understanding of the system.

models that are too general = underparameterized 

models that are too complex = overparameterized

important to consider what the goal of model selection is

- predicting unobserved information? 
- goal is understanding relationships within the data you’ve already collected ?



## Estimation

Estimation is how we use our model, or how we allow the parameters to be 
figured out. three common estimation types:

1. least used **Monte Carlo** estimation (`bootstrapping` or resampling), uses observed data repeatedly to draw inferences

2. most common, "Fisherian estimation" Frequentist estimation assumes a parametric distribution and is interested in the long run of frequencies within the data. 

3. **Bayesian** estimation, Bayesian inference is also assume a parametric distribution. Bayesian estimation includes a prior distribution or prior knowledge about the parameter.


*most important recommendation is to report your estimation type in sufficient detail*


### Process

null hypothesis = no difference from what occurs in nature

1. collect data
2. develop test statistic
3. randomize data and generate large number of test statistics from randomized data
4. randomized test stats create null distribution
5. compare to observed test stats


 **Frequentist** approaches asks: “What is the probability of the data that I observed compared to a fixed and unknowable parameter(s).”
 
paradigm:
 
1. Assume that data are repeatable random variables.
2. Assume that parameters do not change (fixed and unknowable)
3. All experiments are considered to be independent
4. accept or reject hypotheses and outcomes
5. *p-values* are a key outcome


**Bayesian** approach asks “What is the probability of certain parameter values based on the data I observed.”

paradigm:

1. Assume that the data are fixed
2. Adopt a degree-of-belief from probability
3. *Can update beliefs* in the sense that prior information can be used to directly inform the estimate of certain parameters
4. Bayesian estimation is driven by distributions and uncertainty
5. Outcomes are not required to accept or reject anything based on a critical value


<img height='200' src="FreqBayes.png">


The problem with *p-values* 

- often hard to explain 
- p-value does not indicate result is correct or not
- does not provide the magnitude of an effect


### Estimation mechanics

**Frequentist**s use maximum likelihood estimation (MLE), max the log function by minimizing the negative log likelihood 

**Bayesian** uses Bayes rule, a term for posterior distribution, likelihood, prior distribution and normalizing constant

Posterior (outcome) = likelihood * prior / normalizing * constant




# Linear Model 

statistical model vs statistical estimation

## Terms

- units – observations or data points, often indexed with *i*
- $x$ variable = predictors or independent variable
- $y$ variable = outcome, response or dependent variable 
- inputs (model terms) inputs == predictors but predictors are indep. variables
- residual error = the final term of model equation (the unexplained)
- multiple regression is more than 1 predictor (multivariate regression/ MANOVA/ PCA)



## Components

Linear model :

- response = deterministic + stochastic 
- outcome = linear predictor + error
- result = systematic + chance

`lm( y ~ x)` the error term is estimated but not specified

 It is critical that we understand how our (response) data were generated and sampled, in order to best inform our decision about an associated distribution.
 
 1. distributions: normal, binomial. Bernoulli, Poisson and beta, etc
 2. how data was sampled: 
    - if data observations were success/fail then you are working with a Bernoulli distribution
    - if data represents counts >= 0 integers then Poisson distribution
3. data characteristics: what the data looks like
4. run a model & explore distributions
5. no perfect answer: *The goal is always to get to that one, clear distribution that makes sense and is defensible*


Linear component, doesn't always mean straight line (GLMs), predictors can be continuous or categorical. 

- Continuous => regression. 
- Categorical => ANOVA. 
- Continuous & Categorical => ANCOVA (analysis of co-variance)
- design matrix helps understand how the model works, tells you if an effect is present and by how much. use `model.matrix()`


**Parameterization** effect is the intercept estimate is always the mean of group 1, 
but also the baseline estimate that other groups needs to be combined with 



### Distributions

most common ones

1. Normal distribution 

  - for continuous data
  - negative infinity to positive infinity
  - impacted by central limit theorem
  - parameters: mean and variance

2. Binomial distribution

  - discrete integer data (successes out of trial)
  - parameters: p success probability and N sample size
  - Bernoulli distribution N= 1 (coin flip = heads or tails)

3. Poisson distribution

  - counts >= 0 to infinity
  - parameter: mean, variance
  - approx to Binomial when N is large, p is small
  - approx to Normal when lambda is large




### Linear Model - 1

Fake dataset about fish, length is response variable y and other variables are predictors. 

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
fish_df = tibble(
  weight = c(16,18,15,27,29,31),
  stream = factor(c(1,1,2,2,3,3)), # c(1,1,2,2,3,3),
  watershed = factor(c(1,1,1,2,2,2)), # c(1,1,1,2,2,2), 
  habitat = c(1,2,3,1,2,3), #factor(c(1,2,3,1,2,3)),
  length =  c(41,46,37,52,54,60)
)

lm(fish_df$length ~ 1)  # same as mean(length) but in the model compares columns
m= fish_df$length ~ 1
model.matrix(m)
```


### T-test (ANOVA)

Compare 2 groups for lengths = ANOVA test.

```{r}
#                     intercept 1
lm(fish_df$length ~ fish_df$watershed)
```
The mean of `watershed2` is 14 but not found in data, it needs to be added to the estimate of the 1st `watershed` group, thus `41.33 + 14 = 53.33` is the mean length of fish in `watershed2`. *Think of the intercept as a control group, the other columns are the difference between the controls.*


```{r}
# means parameterization 
lm(fish_df$length ~ fish_df$watershed -1)
```

```{r}

model.matrix(fish_df$length ~ fish_df$watershed -1)
```


# Simple Linear Regression

The scatterplot with line of best fit, no categorical data, x changes so does y.
As seen the formula is `lm(length ~ weight)`.


```{r}
lm(fish_df$length ~ fish_df$weight)
```

The fish weight estimate of 1.167 (slope coefficient). For every 1 unit increase in weight the fish length increases by 1.167. [fish length increases 1.167 for every 1 unit in weight].


# Pre-fitting your model

The pre and post fitting of models are a backandforth activity. 

- data transformation, get the data to better fit for a model, such as `log()` values for constraining clustered or skewed data for estimation, or **mean-centering** where you subtract the mean of all observations which centers the data at 0. Avoid test of normality such as Shapiro-Wilk test, Anderson-Darling test etc as they are very conservative and state that data is not normally distributed.

- correlated predictors, ensure predictors are not too correlated => poor coefficient estimates. Correlations closer to 1 mean the 2 variables are highly predictive of each other. There is no set threshold for correlations but `0.6-0.7` should get your attention, below `0.6` is fine. Use the `cor(matrix.object)` for correlation values



# Post-fitting your model

Evaluate the model with the summary function

```{r}
set.seed(15)

n <- 15 # Number of observations
a <- 0 # Intercept
b <- 1.5 # Slope
sigma2 <- 25 # Residual variance
x <- 1:15 # Values of covariate 

eps <-rnorm(n, mean=0, sd=sqrt(sigma2))
y <- a + b*x + eps

model = lm(y ~ x)
summary(model)
```

## Model Summary


- **residuals** = distance from each observation is from the fitted model. Residuals are normally distributed around 0, if not it shows model is over or under predicting too many points. Want median residual ~0, and min and max to be the same

- **coefficients** = parameter estimates

- **significance codes** = the p-values, first select a significance threshold such as alpha = 0.05

- **residual standard error** = the standard deviation (spread) of residuals

- **multiple R-squared** = the % of variance explained by the model, a common model metric, higher values = 'better' but increases with additional model terms

- **adjusted R-squared** = the % of variance explained by the model djusted for more model terms 

- **F-statistic** = tests the null hypothesis that all model coefficients are 0. F-stats are reported for ANOVA models.



```{r}
# plot(model)

# residuals.lm(model) == resid(model)
res = residuals.lm(model)

hist(res, breaks= 10, las= 1, border='black',col= 1, xlab="Residual")
```


### Notes

GOAL: looking for the model to have `homoscedasticity` (means residuals are approximately the same), we don't want patterns in the data == `heteroscedasticity`

For multiple comparisons using ANOVA comes with multiple tests (post hoc means tests) where you run the risk of false positives with increased thresholds. Use **Tukey's Honest Significant Difference** (HSD) test as it is common and versatile.

**ANCOVA** analysis of covariance,  can increase the power to detect differences by reducing the within group variances. It can often be evaluated early as diagnostic to understand the relationship between the continuous and categorical predictors, and the significance of the interaction term may determine how you proceed.

- if the interaction is not significant => remove it









# ANOVA 























