---
title: "Data Analysis"
output: 
  html_document:
    toc: yes
    code_folding: show
    theme:
      bg: "#202123"
      fg: "#c8cbd0" # grey
      primary: "#00b36b"  # pink "#EA80FC" 
      base_font:
        google: Ubuntu
      heading_font:
        google: Ubuntu
      version: 3
---

The source of learning material is **Data Analysis in R** eBook.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (requireNamespace("thematic")) 
  thematic::thematic_rmd(font = "auto")
```



# Models & Estimation

Models are description of the system, process, or relationship you are trying to evaluate. If you have an accurate model for your system, then you will have a better idea of the quality of your data, the inference of your parameters, and generally get much farther in your analysis. 

response  <= (model or relationship being tested or confirmed) + stochastic


**stochastic** = randomly determined, random error or deviations 
from the relationship that are unexplained by the model


Example: data regarding rainfall and plant growth.

- response is $y$ plant growth
- data determines rainfall $x$ 
- hypothesis = increase in rainfall results in increase plant growth
- factors = sunlight, temperature etc.  
- factors and unknowns => random error (stochastic)

Simple Linear Regression: 
as x increase = change in y (constant across all values of x) 

findings:  increase in rainfall results in a small increase in plant growth
which estimates a parameter {coefficient}


## Model Complexity 

We want a model to be general enough that it contains truths and can be applied elsewhere, but not so general that it does not advance our understanding of the system.

models that are too general = underparameterized 

models that are too complex = overparameterized

important to consider what the goal of model selection is

- predicting unobserved information? 
- goal is understanding relationships within the data you’ve already collected ?



## Estimation

Estimation is how we use our model, or how we allow the parameters to be 
figured out. three common estimation types:

1. least used **Monte Carlo** estimation (`bootstrapping` or resampling), uses observed data repeatedly to draw inferences

2. most common, "Fisherian estimation" Frequentist estimation assumes a parametric distribution and is interested in the long run of frequencies within the data. 

3. **Bayesian** estimation, Bayesian inference is also assume a parametric distribution. Bayesian estimation includes a prior distribution or prior knowledge about the parameter.


*most important recommendation is to report your estimation type in sufficient detail*


### Process

null hypothesis = no difference from what occurs in nature

1. collect data
2. develop test statistic
3. randomize data and generate large number of test statistics from randomized data
4. randomized test stats create null distribution
5. compare to observed test stats


 **Frequentist** approaches asks: “What is the probability of the data that I observed compared to a fixed and unknowable parameter(s).”
 
paradigm:
 
1. Assume that data are repeatable random variables.
2. Assume that parameters do not change (fixed and unknowable)
3. All experiments are considered to be independent
4. accept or reject hypotheses and outcomes
5. *p-values* are a key outcome


**Bayesian** approach asks “What is the probability of certain parameter values based on the data I observed.”

paradigm:

1. Assume that the data are fixed
2. Adopt a degree-of-belief from probability
3. *Can update beliefs* in the sense that prior information can be used to directly inform the estimate of certain parameters
4. Bayesian estimation is driven by distributions and uncertainty
5. Outcomes are not required to accept or reject anything based on a critical value


<img height='200' src="FreqBayes.png">


The problem with *p-values* 

- often hard to explain 
- p-value does not indicate result is correct or not
- does not provide the magnitude of an effect


### Estimation mechanics

**Frequentist**s use maximum likelihood estimation (MLE), max the log function by minimizing the negative log likelihood 

**Bayesian** uses Bayes rule, a term for posterior distribution, likelihood, prior distribution and normalizing constant

Posterior (outcome) = likelihood * prior / normalizing * constant




# Linear Model 

statistical model vs statistical estimation

## Terms

- units – observations or data points, often indexed with *i*
- $x$ variable = predictors or independent variable
- $y$ variable = outcome, response or dependent variable 
- inputs (model terms) inputs == predictors but predictors are indep. variables
- residual error = the final term of model equation (the unexplained)
- multiple regression is more than 1 predictor (multivariate regression/ MANOVA/ PCA)



## Components

Linear model :

- response = deterministic + stochastic 
- outcome = linear predictor + error
- result = systematic + chance

`lm( y ~ x)` the error term is estimated but not specified

 It is critical that we understand how our (response) data were generated and sampled, in order to best inform our decision about an associated distribution.
 
 1. distributions: normal, binomial. Bernoulli, Poisson and beta, etc
 2. how data was sampled: 
    - if data observations were success/fail then you are working with a Bernoulli distribution
    - if data represents counts >= 0 integers then Poisson distribution
3. data characteristics: what the data looks like
4. run a model & explore distributions
5. no perfect answer: *The goal is always to get to that one, clear distribution that makes sense and is defensible*


Linear component, doesn't always mean straight line (GLMs), predictors can be continuous or categorical. 

- Continuous => regression. 
- Categorical => ANOVA. 
- Continuous & Categorical => ANCOVA (analysis of co-variance)
- design matrix helps understand how the model works, tells you if an effect is present and by how much. use `model.matrix()`


**Parameterization** effect is the intercept estimate is always the mean of group 1, 
but also the baseline estimate that other groups needs to be combined with 



### Distributions

most common ones

1. Normal distribution 

  - for continuous data
  - negative infinity to positive infinity
  - impacted by central limit theorem
  - parameters: mean and variance

2. Binomial distribution

  - discrete integer data (successes out of trial)
  - parameters: p success probability and N sample size
  - Bernoulli distribution N= 1 (coin flip = heads or tails)

3. Poisson distribution

  - counts >= 0 to infinity
  - parameter: mean, variance
  - approx to Binomial when N is large, p is small
  - approx to Normal when lambda is large




### Linear Model - 1

Fake dataset about fish, length is response variable y and other variables are predictors. 

```{r}
fish_df = tibble(
  weight = c(16,18,15,27,29,31),
  stream = factor(c(1,1,2,2,3,3)),
  watershed = factor(c(1,1,1,2,2,2)),
  habitat = factor(c(1,2,3,1,2,3)),
  length =  c(41,46,37,52,54,60)
)

lm(fish_df$length ~ 1)  # same as mean(length) but in the model compares columns
m= fish_df$length ~ 1
model.matrix(m)
```
### T-test (ANOVA)

Compare 2 groups for lengths = ANOVA test.

```{r}
#                     intercept 1
lm(fish_df$length ~ fish_df$watershed)
```
The mean of `watershed2` is 14 but not found in data, it needs to be added to the estimate of the 1st `watershed` group, thus `41.33 + 14 = 53.33` is the mean length of fish in `watershed2`. *Think of the intercept as a control group, the other columns are the difference between the controls.*


```{r}
# means parameterization 
lm(fish_df$length ~ fish_df$watershed -1)
```

```{r}

model.matrix(fish_df$length ~ fish_df$watershed -1)
```

































